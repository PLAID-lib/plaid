{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Examples\n",
    "\n",
    "This notebook presents how the provided pipeline blocks can be used in a PCA-GP algorithm. Pipeline block directly link PLAID datasets, and hyperpamater tuning is available using scikit-learn's GridSearchCV or Optuna.\n",
    "\n",
    "We start by a few imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from plaid.bridges.huggingface_bridge import huggingface_dataset_to_plaid, huggingface_description_to_problem_definition\n",
    "from plaid.pipelines.sklearn_block_wrappers import WrappedPlaidSklearnTransformer, WrappedPlaidSklearnRegressor\n",
    "from plaid.pipelines.plaid_blocks import PlaidTransformedTargetRegressor, PlaidColumnTransformer\n",
    "\n",
    "nb_cpus = os.cpu_count()\n",
    "print(\"Number of CPUs:\", nb_cpus)\n",
    "n_processes = max(1, int(nb_cpus / 4))\n",
    "print(\"n_processes:\", n_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the `VKI-LS59` dataset from Hugging Face, and restrict ourselves to the first 24 samples of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset(\"PLAID-datasets/VKI-LS59\", split=\"all_samples\")\n",
    "prob_def = huggingface_description_to_problem_definition(hf_dataset.description)\n",
    "\n",
    "train_split = prob_def.get_split(\"train\")[:24]\n",
    "dataset_train, _ = huggingface_dataset_to_plaid(hf_dataset, ids = train_split, processes_number = n_processes, verbose = False)\n",
    "\n",
    "del hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the summary of dataset_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 24 samples, with 8 scalars and 8 fields, which is consistent with the VKI-LS59` dataset. To contain memory consumption, we restrict the dataset to the features required for this example. Far praticity, in_features_identifiers and out_features_identifiers of each pipeline block are defined in a ``.yml`` file. In this example, we try to predict the ``mach`` based on two input scalars ``angle_in`` and ``mach_out``, and the mesh node coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_pipeline.yml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "all_feature_id = config['input_scalar_scaler']['in_features_identifiers'] +\\\n",
    "    config['pca_nodes']['in_features_identifiers'] +\\\n",
    "    config['pca_mach']['in_features_identifiers']\n",
    "\n",
    "dataset_train = dataset_train.from_features_identifier(all_feature_id)\n",
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we keep only 2 scalars and 1 field of interest.\n",
    "\n",
    "We now define a preprocessor: a `MinMaxScaler` of the 2 input scalars and a `PCA` on the nodes coordinates of the meshes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PlaidColumnTransformer([\n",
    "    ('input_scalar_scaler', WrappedPlaidSklearnTransformer(MinMaxScaler(), **config['input_scalar_scaler'])),\n",
    "    ('pca_nodes', WrappedPlaidSklearnTransformer(PCA(), **config['pca_nodes'])),\n",
    "], remainder_feature_ids = config['pca_mach']['in_features_identifiers'])\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `PlaidColumnTransformer`, which enable independant transformations of features. The `out_features_identifiers` of each transformer are appended to `remainder_feature_ids`, which specifies the feature that will be passed through, such that only these features are kept in the returned merged dataset.\n",
    "\n",
    "We check this by applying the `preprocessor` to `dataset_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = preprocessor.fit_transform(dataset_train)\n",
    "print(preprocessed_dataset)\n",
    "print(\"scalar names =\", preprocessed_dataset.get_scalar_names())\n",
    "print(\"field names =\", preprocessed_dataset.get_field_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `MinMaxScaler`, we have scaled `angle_in` and `mach_out` and overridden their values, while with `PCA`, we have compressed the nodes coordinates and returned scalars with name `reduced_nodes_*'` containing the PCA coordinates. We could have specified `out_features_identifiers` in the `.yml` file to generate new scalars instead of overriding `in_features_identifiers`.\n",
    "\n",
    "We now define the postprocessor, which is here a PCA on the `mach` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = WrappedPlaidSklearnTransformer(PCA(), **config['pca_mach'])\n",
    "postprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regressor in a Gaussian Process applied on the transformed ``angle_in`` and ``mach_out``, and the mesh node coordinates PCA coefficients as inputs, and the ``mach`` PCA coefficients as outputs. A ``PlaidTransformedTargetRegressor`` enable us to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Matern(length_scale_bounds=(1e-8, 1e8), nu = 2.5)\n",
    "\n",
    "gpr = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    optimizer='fmin_l_bfgs_b',\n",
    "    n_restarts_optimizer=1,\n",
    "    random_state=42)\n",
    "\n",
    "reg = MultiOutputRegressor(gpr)\n",
    "\n",
    "def length_scale_init(X):\n",
    "    return np.ones(X.shape[1])\n",
    "\n",
    "dynamics_params_factory = {'estimator__kernel__length_scale':length_scale_init}\n",
    "\n",
    "regressor = WrappedPlaidSklearnRegressor(reg, **config['regressor_mach'], dynamics_params_factory = dynamics_params_factory)\n",
    "\n",
    "target_regressor = PlaidTransformedTargetRegressor(\n",
    "    regressor=regressor,\n",
    "    transformer=postprocessor,\n",
    "    transformed_target_feature_id=config['pca_mach']['in_features_identifiers']\n",
    ")\n",
    "target_regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PlaidTransformedTargetRegressor` work as a scikit-learn `TransformedTargetRegressor`, but directly on PLAID datasets. The argument `transformed_target_feature_id` allows to specify which feature identifiers are concerned by the transformation.\n",
    "\n",
    "Finally, we define the complete pipeline as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", target_regressor),\n",
    "    ]\n",
    ")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna\n",
    "\n",
    "We now use optune to optimze the hyperparmeters, by a research over the number of components of the two `PCA` blocks, and a three-fold cross validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    nodes_n_components = trial.suggest_int(\"preprocessor__pca_nodes__sklearn_block__n_components\", 3, 4)\n",
    "    mach_n_components = trial.suggest_int(\"regressor__transformer__sklearn_block__n_components\", 4, 5)\n",
    "\n",
    "    # Clone and configure pipeline\n",
    "    pipeline_run = clone(pipeline)\n",
    "    pipeline_run.set_params(\n",
    "        preprocessor__pca_nodes__sklearn_block__n_components=nodes_n_components,\n",
    "        regressor__transformer__sklearn_block__n_components=mach_n_components\n",
    "    )\n",
    "\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    indices = np.arange(len(dataset_train))\n",
    "\n",
    "    for train_idx, val_idx in cv.split(indices):\n",
    "\n",
    "        dataset_cv_train_ = dataset_train[train_idx]\n",
    "        dataset_cv_val_   = dataset_train[val_idx]\n",
    "\n",
    "        pipeline_run.fit(dataset_cv_train_)\n",
    "\n",
    "        score = pipeline_run.score(dataset_cv_val_)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maximize the defined objective function over 4 trial runs chosen by optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=4)\n",
    "print(\"best_params =\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the best found hyperparameters and define `optimized_pipeline` based on these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_pipeline = clone(pipeline).set_params(**study.best_params)\n",
    "optimized_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the `dataset_train` dataset and compute the score on this same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_pipeline.fit(dataset_train)\n",
    "dataset_pred = optimized_pipeline.predict(dataset_train)\n",
    "score = optimized_pipeline.score(dataset_train)\n",
    "print(\"score =\", score, \", error =\", 1. - score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an anisotropic kernel in the Gaussian Process: its optimized `length_scale` should be a vector with 2+preprocessor__pca_nodes__sklearn_block__n_components components (since we have appending 2 input scalars):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension GP kernel length_scale =\", len(optimized_pipeline.named_steps[\"regressor\"].regressor_.sklearn_block_.estimators_[0].kernel_.get_params()['length_scale']))\n",
    "print(\"Expected dimension =\", 2 + study.best_params['preprocessor__pca_nodes__sklearn_block__n_components'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is non-zero on due to the PCA errors. Since we have an interpolating Gaussian Process, we expect the error to vanish on the training set if we keep all the PCA modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_pipeline = clone(pipeline).set_params(\n",
    "    preprocessor__pca_nodes__sklearn_block__n_components = 24,\n",
    "    regressor__transformer__sklearn_block__n_components = 24\n",
    ")\n",
    "exact_pipeline.fit(dataset_train)\n",
    "dataset_pred = exact_pipeline.predict(dataset_train)\n",
    "score = exact_pipeline.score(dataset_train)\n",
    "print(\"score =\", score, \", error =\", 1. - score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "Our pipeline node design satisfying the scikit-learn API, the constructed pipeline is directly compatible with GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'preprocessor__pca_nodes__sklearn_block__n_components': [3, 4],\n",
    "    'regressor__transformer__sklearn_block__n_components': [4, 5],\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipeline, param_grid=param_grid, cv=3, verbose=3, error_score='raise')\n",
    "search.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the score on the training set using the optimized pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_params =\", search.best_params_)\n",
    "optimized_pipeline = clone(pipeline).set_params(**search.best_params_)\n",
    "optimized_pipeline.fit(dataset_train)\n",
    "dataset_pred = optimized_pipeline.predict(dataset_train)\n",
    "score = optimized_pipeline.score(dataset_train)\n",
    "print(\"score =\", score, \", error =\", 1. - score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plaid-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
