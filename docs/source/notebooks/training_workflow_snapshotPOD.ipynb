{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING ONLY, NOT SUITABLE FOR PUBLIC OPEN SOURCE (local paths included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple of training workflow using a sklearn `Pipeline`\n",
    "\n",
    "The workflow is composed of the following steps:\n",
    "- read data\n",
    "- basic analysis of data\n",
    "- define pipeline\n",
    "- train and evaluate predictions\n",
    "- Cross-Validation (CV), here with `KFold`\n",
    "- Find optimal parameters, here with `GridSearchCV`\n",
    "\n",
    "This workflow takes around 5min to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "### basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import rich\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classes to implement our own transforms and models\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.base import ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator\n",
    "\n",
    "# transforms, preprocessings and models\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# composition of transforms and models\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import r2_score #, root_mean_squared_error\n",
    "\n",
    "# hp-tuning and validation\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAID imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../../src')\n",
    "from plaid.containers.sample import Sample\n",
    "from plaid.containers.dataset import Dataset\n",
    "from plaid.problem_definition import ProblemDefinition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gpfs_new/cold-data/InputData/public_datasets/AI4Design/AirfRANS/converted_plaid/meshes_cliped_tri')\n",
    "# data_dir = Path(os.environ['AIRFRANS_PLAID_DATASET_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(data_dir/'dataset')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_definition = ProblemDefinition(data_dir/'problem_definition')\n",
    "problem_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = problem_definition.get_split()\n",
    "splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scalars_names = problem_definition.get_input_scalars_names()\n",
    "print(f\"{input_scalars_names=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_scalars_names = problem_definition.get_output_scalars_names()\n",
    "print(f\"{output_scalars_names=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = dataset.get_scalars_to_tabular()\n",
    "scalars.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([scalars[isn].reshape((-1,1)) for isn in input_scalars_names], axis=1)\n",
    "y = np.concatenate([scalars[osn].reshape((-1,1)) for osn in output_scalars_names], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[splits['full_train']]\n",
    "y_train = y[splits['full_train']]\n",
    "X_test = X[splits['full_test']]\n",
    "y_test = y[splits['full_test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars['test'] = np.zeros(len(dataset), dtype=bool)\n",
    "scalars['test'][splits['full_test']] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars_pd = pd.DataFrame(scalars)\n",
    "scalars_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10,6))\n",
    "# for i_feat in range(len(input_scalars_names)):\n",
    "#     isn = input_scalars_names[i_feat]\n",
    "#     ax = fig.add_subplot(1,len(input_scalars_names),i_feat+1)\n",
    "#     sns.violinplot(data=scalars_pd, y=isn, hue=\"test\", split=True, inner='quart', gap=0.1, legend=True)\n",
    "#     ax.set_title(isn)\n",
    "# plt.suptitle('input scalars')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10,6))\n",
    "# for i_feat in range(len(output_scalars_names)):\n",
    "#     isn = output_scalars_names[i_feat]\n",
    "#     ax = fig.add_subplot(1,len(output_scalars_names),i_feat+1)\n",
    "#     sns.violinplot(data=scalars_pd, y=isn, hue=\"test\", split=True, inner='quart', gap=0.1, legend=True)\n",
    "#     ax.set_title(isn)\n",
    "# plt.suptitle('output scalars')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of output scalar C_D is squashed close to 0, a log transform should better spread the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars_pd['C_D'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalars_pd['log_C_D'] = np.log10(scalars_pd['C_D'])\n",
    "# fig = plt.figure(figsize=(6,6))\n",
    "# sns.violinplot(data=scalars_pd, y='log_C_D', hue=\"test\", split=True, inner='quart', gap=0.1, legend=True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseTransform(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,*args,**kargs):\n",
    "        return self\n",
    "    def transform(self,x):\n",
    "        return np.concatenate([x,1 / x], axis=1)\n",
    "    def inverse_transform(self,x):\n",
    "        return x[:,:x.shape[1]//2]\n",
    "\n",
    "#---# this methods are useless as we inherit from BaseEstimator\n",
    "    # def get_params(self,*args,**kargs):\n",
    "    #     return {'scale':self.scale, 'offset':self.offset}\n",
    "        # return {'scale':self.scale, 'inv_scale':self.inv_scale, 'offset':self.offset}\n",
    "    # def set_params(self, **parameters):\n",
    "    #     for parameter, value in parameters.items():\n",
    "    #         setattr(self, parameter, value)\n",
    "    #     return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnapshotPOD(\n",
    "    ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator\n",
    "):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_components=None,\n",
    "        correlationOperator = None):\n",
    "        self.n_components = n_components\n",
    "        self.correlationOperator = correlationOperator\n",
    "        self.reducedOrderBasis = None\n",
    "\n",
    "    def TruncatedSVDSymLower(self, matrix, epsilon = None, nbModes = None):\n",
    "\n",
    "        if epsilon != None and nbModes != None:\n",
    "            raise(\"cannot specify both epsilon and nbModes\")\n",
    "\n",
    "        eigenValues, eigenVectors = np.linalg.eigh(matrix, UPLO=\"L\")\n",
    "\n",
    "        idx = eigenValues.argsort()[::-1]\n",
    "        eigenValues = eigenValues[idx]\n",
    "        eigenVectors = eigenVectors[:, idx]\n",
    "\n",
    "        if nbModes == None:\n",
    "            if epsilon == None:\n",
    "                nbModes  = matrix.shape[0]\n",
    "            else:\n",
    "                nbModes = 0\n",
    "                bound = (epsilon ** 2) * eigenValues[0]\n",
    "                for e in eigenValues:\n",
    "                    if e > bound:\n",
    "                        nbModes += 1\n",
    "                id_max2 = 0\n",
    "                bound = (1 - epsilon ** 2) * np.sum(eigenValues)\n",
    "                temp = 0\n",
    "                for e in eigenValues:\n",
    "                    temp += e\n",
    "                    if temp < bound:\n",
    "                        id_max2 += 1\n",
    "\n",
    "                nbModes = max(nbModes, id_max2)\n",
    "\n",
    "        if nbModes > matrix.shape[0]:\n",
    "            print(\"nbModes taken to max possible value of \"+str(matrix.shape[0])+\" instead of provided value \"+str(nbModes))\n",
    "            nbModes = matrix.shape[0]\n",
    "\n",
    "        index = np.where(eigenValues<0)\n",
    "        if len(eigenValues[index])>0:\n",
    "            if index[0][0]<nbModes:\n",
    "                print(\"removing numerical noise from eigenvalues, nbModes is set to \"+str(index[0][0])+\" instead of \"+str(nbModes))\n",
    "                nbModes = index[0][0]\n",
    "\n",
    "        return eigenValues[0:nbModes], eigenVectors[:, 0:nbModes]\n",
    "\n",
    "\n",
    "    def compute_matVecProducts(self, X):\n",
    "        numberOfSnapshots = X.shape[0]\n",
    "        numberOfDofs = X.shape[1]\n",
    "\n",
    "        if self.correlationOperator == None:\n",
    "            correlationOperator = sparse.eye(numberOfDofs)\n",
    "        else:\n",
    "            correlationOperator = self.correlationOperator\n",
    "\n",
    "        matVecProducts = np.zeros((numberOfDofs,numberOfSnapshots))\n",
    "        for i, snapshot1 in enumerate(X):\n",
    "            matVecProduct = correlationOperator.dot(snapshot1)\n",
    "            matVecProducts[:,i] = matVecProduct\n",
    "\n",
    "        return matVecProducts\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        numberOfSnapshots = X.shape[0]\n",
    "        correlationMatrix = np.zeros((numberOfSnapshots,numberOfSnapshots))\n",
    "        matVecProducts = self.compute_matVecProducts(X)\n",
    "\n",
    "        for i in range(numberOfSnapshots):\n",
    "            matVecProduct = matVecProducts[:,i]\n",
    "            for j, snapshot2 in enumerate(X):\n",
    "                if j <= i and j < numberOfSnapshots:\n",
    "                    correlationMatrix[i, j] = np.dot(matVecProduct, snapshot2)\n",
    "\n",
    "        eigenValuesRed, eigenVectorsRed = self.TruncatedSVDSymLower(correlationMatrix, nbModes = self.n_components)\n",
    "\n",
    "        nbePODModes = eigenValuesRed.shape[0]\n",
    "        # print(\"truncature =\", eigenValuesRed[-1]/eigenValuesRed[0])\n",
    "\n",
    "        changeOfBasisMatrix = np.zeros((nbePODModes,numberOfSnapshots))\n",
    "        for j in range(nbePODModes):\n",
    "            changeOfBasisMatrix[j,:] = eigenVectorsRed[:,j]/np.sqrt(eigenValuesRed[j])\n",
    "\n",
    "        self.reducedOrderBasis = np.dot(changeOfBasisMatrix, X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        matVecProducts = self.compute_matVecProducts(X)\n",
    "        return np.dot(self.reducedOrderBasis, matVecProducts).T\n",
    "\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "\n",
    "        numberOfDofs = X.shape[1]\n",
    "        if self.correlationOperator == None:\n",
    "            correlationOperator = sparse.eye(numberOfDofs)\n",
    "        else:\n",
    "            correlationOperator = self.correlationOperator\n",
    "        return np.dot(self.reducedOrderBasis, correlationOperator.dot(X.T)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pca_components = 32\n",
    "nb_trees = 1024\n",
    "\n",
    "inverse = FeatureUnion([\n",
    "    ('identity', 'passthrough'),\n",
    "    ('inverse', FunctionTransformer(func=(lambda x:1.0/x), inverse_func=(lambda x:1.0/x))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "regressor = Pipeline([\n",
    "                      ('inverse',inverse),\n",
    "                      ('union',FeatureUnion([\n",
    "                                             ('reduced_polynom',Pipeline([\n",
    "                                                                          ('polynom',PolynomialFeatures(degree=3, include_bias=False)),\n",
    "                                                                          ('snapshotPOD',SnapshotPOD(n_components=8)),\n",
    "                                                                         ])),\n",
    "                                             ('kbins',KBinsDiscretizer()),\n",
    "                                            ])),\n",
    "                      ('scaler',RobustScaler(with_centering=False)),\n",
    "                      # ('pca',PCA(n_components=nb_pca_components)),\n",
    "                      ('model',RandomForestRegressor(n_estimators=nb_trees,\n",
    "                                                     oob_score=True,\n",
    "                                                     n_jobs=4,\n",
    "                                                     verbose=0,\n",
    "                                                    )),\n",
    "                     ])\n",
    "regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen previously, to better spread the distribution of output scalar C_D, we apply a LogTransform to this output \n",
    "This is done using: \n",
    "- ``TransformedTargetRegressor``, that transform a target before trying to fit a regressor to it\n",
    "- ``InvertibleColumnTransformer``, that apply a transform (here ``LogTransform``) to only a chosen set of columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransform(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self,scale=1.0,offset=0.0):\n",
    "        self.scale = scale\n",
    "        self.inv_scale = 1.0/scale\n",
    "        self.offset = offset\n",
    "    def fit(self,*args,**kargs):\n",
    "        return self\n",
    "    def transform(self,x):\n",
    "        nb_samples = x.shape[0]\n",
    "        return np.log(self.scale * x + self.offset).reshape((nb_samples,-1))\n",
    "    def inverse_transform(self,x):\n",
    "        nb_samples = x.shape[0]\n",
    "        return self.inv_scale * (np.exp(x)-self.offset).reshape((nb_samples,-1))\n",
    "\n",
    "#---# this methods are useless as we inherit from BaseEstimator\n",
    "    # def get_params(self,*args,**kargs):\n",
    "    #     return {'scale':self.scale, 'offset':self.offset}\n",
    "        # return {'scale':self.scale, 'inv_scale':self.inv_scale, 'offset':self.offset}\n",
    "    # def set_params(self, **parameters):\n",
    "    #     for parameter, value in parameters.items():\n",
    "    #         setattr(self, parameter, value)\n",
    "    #     return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertibleColumnTransformer(ColumnTransformer):\n",
    "    \"\"\"\n",
    "    Adds an inverse transform method to the standard sklearn.compose.ColumnTransformer.\n",
    "\n",
    "    Warning this is flaky and use at your own risk.  Validation checks that the column count in\n",
    "    `transformers` are in your object `X` to be inverted.  Reordering of columns will break things!\n",
    "    \"\"\"\n",
    "    def inverse_transform(self, X):\n",
    "\n",
    "        # check that remainder is set to 'passthrough' (only if there are remaining columns)\n",
    "        if len(self._remainder[2])==0 or self._remainder[1] == 'passthrough':\n",
    "\n",
    "            all_input_ids = list(self._transformer_to_input_indices.values())\n",
    "            all_input_ids = sorted(np.concatenate(all_input_ids).flatten())\n",
    "\n",
    "            # check that all columns are treated, and only ones\n",
    "            if np.all(np.arange(self.n_features_in_) == all_input_ids):\n",
    "                #------------------------------------------------------------------------------------------------#\n",
    "                #---# From https://github.com/scikit-learn/scikit-learn/issues/11463#issuecomment-1674435238 #---#\n",
    "                #------------------------------------------------------------------------------------------------#\n",
    "                if isinstance(X,pd.DataFrame):\n",
    "                    X = X.to_numpy()\n",
    "\n",
    "                arrays = []\n",
    "                for name, indices in self.output_indices_.items():\n",
    "                    transformer = self.named_transformers_.get(name, None)\n",
    "                    arr = X[:, indices.start: indices.stop]\n",
    "\n",
    "                    if transformer is None:\n",
    "                        pass\n",
    "\n",
    "                    else:\n",
    "                        arr = transformer.inverse_transform(arr)\n",
    "\n",
    "                    arrays.append(arr)\n",
    "\n",
    "                retarr = np.concatenate(arrays, axis=1)\n",
    "\n",
    "                if retarr.shape[1] != X.shape[1]:\n",
    "                    raise ValueError(f\"Received {X.shape[1]} columns but transformer expected {retarr.shape[1]}\")\n",
    "\n",
    "                return retarr\n",
    "                #------------------------------------------------------------------------------------------------#\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"only {all_input_ids} input columns are treated, but all input columns should be treated (there are {self.n_features_in_})\")\n",
    "        else:\n",
    "            raise ValueError(f\"remainder set to '{self._remainder[1]}' but should be 'passthrough'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = TransformedTargetRegressor(regressor=regressor)\n",
    "pipeline = TransformedTargetRegressor(regressor=regressor, transformer=InvertibleColumnTransformer([('log_C_D',LogTransform(),0)], remainder='passthrough'))\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.regressor_[-1].oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oob_pred_train = pipeline.transformer_.inverse_transform(pipeline.regressor_[-1].oob_prediction_)\n",
    "r2_score(y_train, oob_pred_train, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pipeline.predict(X_train)\n",
    "pred_test = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, pred_train, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, pred_test, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold Cross-Validation\n",
    "\n",
    "There are many cross-validation (CV) methods, will see here the simplest: KFold cross-validation.\n",
    "\n",
    "For a comparison of different CV methods (in the case of classification) see: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = []\n",
    "oob = []\n",
    "for i, (train_index, test_index) in tqdm(enumerate(kf.split(X_train)), desc='Treat KFold', total=kf.get_n_splits()):\n",
    "    # print(f\"--- Fold {i}:\")\n",
    "    # print(f\"  {len(train_index)=} -> {train_index[:10]}\")\n",
    "    # print(f\"  {len(test_index)=}  -> {test_index[:10]}\")\n",
    "\n",
    "    inverse = FeatureUnion([\n",
    "        ('identity', 'passthrough'),\n",
    "        ('inverse', FunctionTransformer(func=(lambda x:1.0/x), inverse_func=(lambda x:1.0/x))),\n",
    "    ])\n",
    "\n",
    "    regressor = Pipeline([\n",
    "                      ('inverse',inverse),\n",
    "                      ('union',FeatureUnion([\n",
    "                                             ('reduced_polynom',Pipeline([\n",
    "                                                                          ('polynom',PolynomialFeatures(degree=3, include_bias=False)),\n",
    "                                                                          ('snapshotPOD',SnapshotPOD(n_components=8)),\n",
    "                                                                         ])),\n",
    "                                             ('kbins',KBinsDiscretizer()),\n",
    "                                            ])),\n",
    "                      ('scaler',RobustScaler(with_centering=False)),\n",
    "                      # ('pca',PCA(n_components=nb_pca_components)),\n",
    "                      ('model',RandomForestRegressor(n_estimators=nb_trees,\n",
    "                                                     oob_score=True,\n",
    "                                                     n_jobs=4,\n",
    "                                                     verbose=0,\n",
    "                                                    )),\n",
    "                     ])\n",
    "    pipeline = TransformedTargetRegressor(regressor=regressor, transformer=InvertibleColumnTransformer([('log_C_D',LogTransform(),0)], remainder='passthrough'))\n",
    "\n",
    "    # train\n",
    "    pipeline.fit(X_train[train_index], y_train[train_index])\n",
    "\n",
    "    oob.append(pipeline.regressor_[-1].oob_score_)\n",
    "\n",
    "    # evaluate\n",
    "    y_true = y_train[test_index]\n",
    "    y_pred = pipeline.predict(X_train[test_index])\n",
    "    r2.append(r2_score(y_true, y_pred, multioutput='raw_values'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_fold in range(kf.get_n_splits()):\n",
    "    print(f\" - {i_fold} -> r2={r2[i_fold]}, oob={oob[i_fold]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(oob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(r2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(r2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{pipeline.regressor_[:1].transform(X_train).shape=}\")\n",
    "print(f\"{pipeline.regressor_[1]['reduced_polynom'][0].transform(pipeline.regressor_[0].transform(X_train)).shape=}\")\n",
    "print(f\"{pipeline.regressor_[1]['kbins'].transform(pipeline.regressor_[0].transform(X_train)).shape=}\")\n",
    "print(f\"{pipeline.regressor_[:2].transform(X_train).shape=}\")\n",
    "print(f\"{pipeline.regressor_[:3].transform(X_train).shape=}\")\n",
    "print(f\"{pipeline.regressor_.predict(X_train).shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'regressor__model__n_estimators': [64,128,256,512],\n",
    "    'regressor__union__reduced_polynom__polynom__degree': [3,4],\n",
    "    'regressor__union__reduced_polynom__snapshotPOD__n_components': [2,4,8],\n",
    "}\n",
    "grid_cv = GridSearchCV(pipeline, param_grid=param_grid, refit=True, verbose=2)\n",
    "grid_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{grid_cv.cv_results_['mean_fit_time']=}\")\n",
    "# print(f\"{grid_cv.cv_results_['std_fit_time']=}\")\n",
    "# print(f\"{grid_cv.cv_results_['mean_score_time']=}\")\n",
    "# print(f\"{grid_cv.cv_results_['std_score_time']=}\")\n",
    "print(f\"{grid_cv.cv_results_['mean_test_score']=}\")\n",
    "print(f\"{grid_cv.cv_results_['std_test_score']=}\")\n",
    "print(f\"{grid_cv.cv_results_['rank_test_score']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{grid_cv.cv_results_['param_regressor__model__n_estimators']=}\")\n",
    "print(f\"{grid_cv.cv_results_['param_regressor__union__reduced_polynom__snapshotPOD__n_components']=}\")\n",
    "print(f\"{grid_cv.cv_results_['param_regressor__union__reduced_polynom__polynom__degree']=}\")\n",
    "print(f\"{grid_cv.cv_results_['params']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_case_id = np.argmin(grid_cv.cv_results_['rank_test_score'])\n",
    "best_case_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{grid_cv.cv_results_['param_regressor__model__n_estimators'][best_case_id]=}\")\n",
    "print(f\"{grid_cv.cv_results_['param_regressor__union__reduced_polynom__snapshotPOD__n_components'][best_case_id]=}\")\n",
    "print(f\"{grid_cv.cv_results_['param_regressor__union__reduced_polynom__polynom__degree'][best_case_id]=}\")\n",
    "print(f\"{grid_cv.cv_results_['params'][best_case_id]=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_geometry_old_updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
