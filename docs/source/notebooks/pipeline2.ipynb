{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Examples\n",
    "\n",
    "This notebook demonstrates the end-to-end process of building a machine learning pipeline using PLAID datasets and PLAIDâ€™s scikit-learn-compatible blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "warnings.filterwarnings(\"ignore\", message=\".*IProgress not found.*\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "from plaid.bridges.huggingface_bridge import huggingface_dataset_to_plaid, huggingface_description_to_problem_definition\n",
    "from plaid.pipelines.sklearn_block_wrappers import WrappedPlaidSklearnTransformer, WrappedPlaidSklearnRegressor\n",
    "from plaid.pipelines.plaid_blocks import PlaidTransformedTargetRegressor, PlaidColumnTransformer\n",
    "\n",
    "disable_progress_bar()\n",
    "n_processes = min(max(1, os.cpu_count()), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMGP for `U1` field prediction of `Tensile2d` dataset\n",
    "\n",
    "Key steps covered:\n",
    "\n",
    "- **Loading and preparing the PLAID dataset** using Hugging Face integration and PLAIDâ€™s dataset classes  \n",
    "- **Standardizing features** with PLAID-wrapped scikit-learn transformers for scalars and fields  \n",
    "- **Dimensionality reduction** of flow fields via Principal Component Analysis (PCA) to reduce output complexity  \n",
    "- **Regression modeling** of PCA coefficients from scalar inputs using Gaussian Process regression  \n",
    "- **Pipeline assembly** combining transformations and regressors into a single scikit-learn-compatible workflow  \n",
    "- **Hyperparameter tuning** using Optuna and scikit-learnâ€™s `GridSearchCV`\n",
    "- **Model evaluation** using cross-validation and appropriate metrics  \n",
    "- **Best practices** for working with PLAID datasets and pipelines in a reproducible and modular manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¥ Load Dataset\n",
    "\n",
    "We load the `Tensile2d` dataset from Hugging Face and restrict ourselves to the first 24 samples of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset(\"PLAID-datasets/Tensile2d\", split=\"all_samples[:24]\")\n",
    "dataset_train, _ = huggingface_dataset_to_plaid(hf_dataset, processes_number = 6, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset_train =\", dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    filename = Path(__file__).parent.parent.parent / \"docs\" / \"source\" / \"notebooks\" / \"config_pipeline2.yml\"\n",
    "except NameError:\n",
    "    filename = \"config_pipeline2.yml\"\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "all_feature_id = config['input_scalar_scaler']['in_features_identifiers'] +\\\n",
    "    config['pca_nodes']['in_features_identifiers'] + config['pca_u1']['in_features_identifiers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.from_features_identifier(all_feature_id)\n",
    "print(\"dataset_train:\", dataset_train)\n",
    "print(\"scalar names =\", dataset_train.get_scalar_names())\n",
    "print(\"field names =\", dataset_train.get_field_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MM_node import MeshMorphingInterpolationTransformer\n",
    "# mm = MeshMorphingInterpolationTransformer(common_mesh_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm.fit(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_dataset = mm.transform(dataset_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_dataset[0].get_field(\"U1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Muscat.Bridges.CGNSBridge import CGNSToMesh\n",
    "\n",
    "# print(transformed_dataset)\n",
    "# index = 1\n",
    "# print(transformed_dataset[index].get_all_mesh_times())\n",
    "# print(transformed_dataset[index].get_field(\"coords_X\", time = 0.).shape)\n",
    "# print(transformed_dataset.extra_data[index].get_field(\"coords_X\").shape)\n",
    "# print(transformed_dataset[index].get_field(\"U1\", time = 0.).shape)\n",
    "# print(transformed_dataset.extra_data[index].get_field(\"U1\"))\n",
    "# mesh = CGNSToMesh(transformed_dataset[index].get_mesh(0.))\n",
    "# mesh_o = CGNSToMesh(transformed_dataset.extra_data[index].get_mesh())\n",
    "# print(mesh)\n",
    "# print(mesh_o)\n",
    "# print(\">>\", transformed_dataset[0].get_field(\"U1\", time = 0.).shape)\n",
    "# print(\">>\", transformed_dataset[1].get_field(\"U1\", time = 0.).shape)\n",
    "# print(\">>\", transformed_dataset[0].get_nodes().shape)\n",
    "# print(\">>\", transformed_dataset[1].get_nodes().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(CGNSToMesh(dataset_train[0].get_mesh(0.)))\n",
    "# print(CGNSToMesh(dataset_train[1].get_mesh(0.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv_transformed_dataset = mm.inverse_transform(transformed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 0\n",
    "# print(inv_transformed_dataset[index].get_scalar('P'))\n",
    "# print(dataset_train[index].get_scalar('P'))\n",
    "# print(\"==\")\n",
    "# print(np.linalg.norm(inv_transformed_dataset[index].get_nodes()-dataset_train[index].get_nodes()))\n",
    "# print(\"==\")\n",
    "# print(inv_transformed_dataset[index].get_field(\"U1\"))\n",
    "# print(dataset_train[index].get_field(\"U1\"))\n",
    "# print(np.linalg.norm(inv_transformed_dataset[index].get_field(\"U1\")-dataset_train[index].get_field(\"U1\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_preprocessor = MeshMorphingInterpolationTransformer(common_mesh_id = 12)\n",
    "\n",
    "preprocessor = PlaidColumnTransformer(\n",
    "    [\n",
    "        ('input_scalar_scaler', WrappedPlaidSklearnTransformer(MinMaxScaler(), **config['input_scalar_scaler'])),\n",
    "        ('pca_nodes', WrappedPlaidSklearnTransformer(PCA(n_components=24), **config['pca_nodes'])),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = WrappedPlaidSklearnTransformer(PCA(n_components=24), **config['pca_u1'])\n",
    "\n",
    "kernel = Matern(length_scale_bounds=(1e-8, 1e8), nu = 2.5)\n",
    "\n",
    "gpr = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    optimizer='fmin_l_bfgs_b',\n",
    "    n_restarts_optimizer=1,\n",
    "    random_state=42)\n",
    "\n",
    "reg = MultiOutputRegressor(gpr)\n",
    "\n",
    "def length_scale_init(X):\n",
    "    return np.ones(X.shape[1])\n",
    "\n",
    "dynamics_params_factory = {'estimator__kernel__length_scale':length_scale_init}\n",
    "\n",
    "regressor = WrappedPlaidSklearnRegressor(reg, **config['regressor_mach'], dynamics_params_factory = dynamics_params_factory)\n",
    "\n",
    "target_regressor = PlaidTransformedTargetRegressor(\n",
    "    regressor=regressor,\n",
    "    transformer=postprocessor\n",
    ")\n",
    "target_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", target_regressor),\n",
    "    ]\n",
    ")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_1 = mm_preprocessor.fit_transform(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(dataset_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred_1 = pipeline.predict(dataset_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(24):\n",
    "    print(\"rel_dif =\", np.linalg.norm(dataset_pred_1[index].get_field(\"U1\") - dataset_train_1[index].get_field(\"U1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred = mm_preprocessor.inverse_transform(dataset_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(24):\n",
    "    print(\"rel_dif =\", np.linalg.norm(dataset_pred[index].get_field(\"U1\") - dataset_train[index].get_field(\"U1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_pred[0].get_field_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train[0].get_field_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plaid-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
